{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Q-learning\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/endi/.local/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/endi/.local/lib/python3.8/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: numpy in /home/endi/.local/lib/python3.8/site-packages (from torchvision) (1.20.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/lib/python3/dist-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions in /home/endi/.local/lib/python3.8/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#imports used for task2 \n",
    "import torch\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda_available else \"cpu\")\n",
    "print(device)\n",
    "from typing import Tuple\n",
    "from q_maze import QMaze, Action\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from e_greedy_pol import E_greedy_policy\n",
    "import random as random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing action value functions using E_greedy_policy\n",
    "\n",
    "For this method we will use the E_greedy_policy to help us compute and estimate the **q-value** of each state.\n",
    "\n",
    "The **q-value** is the **mean** expected future reward following an action from a given state. Rather than storing all of our experience and taking the mean over them, we can use each experience to update an exponentially weighted average forget that exprience.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epsilon-Greedy is a simple method to balance exploration and exploitation by choosing between exploration and exploitation randomly\n",
    "class E_greedy_policy:\n",
    "    def __init__(self, epsilon, decay):\n",
    "\n",
    "        self.epsilon = epsilon #initial value of epsilon\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay = decay #parameter used to control how much the agent should explore and exploit when using epislon-greedy policy.\n",
    "\n",
    "    # This function is used to select the action with max values \n",
    "    #For us to be able to select max values we need to know the state and the q_values\n",
    "    def __call__(self, state, q_values): \n",
    "\n",
    "        is_greedy = random.random() > self.epsilon\n",
    "\n",
    "        if is_greedy:\n",
    "            # we select a greedy action by getting the max q_values from the grid\n",
    "            action_index = np.argmax(q_values[state])\n",
    "        else:\n",
    "            # else we get a random choice from action\n",
    "            action_index = random.choice(list(Action)).value.index\n",
    "        #while selected_action = None\n",
    "        selected_action = None\n",
    "        #we pick an action from our possible action moves\n",
    "        for a in list(Action):\n",
    "            if a.value.index == action_index:\n",
    "                selected_action = a\n",
    "        return selected_action\n",
    "\n",
    "    #TODO understand what this is doing\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = self.epsilon * self.decay\n",
    "\n",
    "    #TODO do we need this?\n",
    "    def reset(self):\n",
    "        self.epsilon = self.epsilon_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    \"\"\"Instant diff parameters for calc Q\"\"\"\n",
    "    def __init__(self, policy, env, gamma, alpha):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.env = env.size\n",
    "        self.coord_to_index_state = env.coord_to_index_state\n",
    "\n",
    "        self.q_values = np.zeros( (self.env * self.env,(len(list(Action) ))))\n",
    "\n",
    "\n",
    "    #We are updating the values from our q.values table after each step\n",
    "    def update_values(self, state_current, state_next, action, reward):\n",
    "\n",
    "        old_value = self.q_values[state_current, action]\n",
    "        next_max = np.max(self.q_values[state_next])\n",
    "        \n",
    "        #Calculate the new q.values from the q-learning formula\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_values[state_current, action] = new_value\n",
    "\n",
    "\n",
    "    #we are assigning the maximum qvalues to the grid in the maze so we can calculate the optimum route the agent must take in order to maximise reward\n",
    "    def new_values(self):\n",
    "\n",
    "        # Return the index of the best action for each state\n",
    "        values = np.argmax(self.q_values, axis = 1)\n",
    "        \n",
    "\n",
    "        return values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X X X X X X X X X X X X X X \n",
      "X . . . . . . . . . X . X . A . X X . X \n",
      "X . X . X X . X . X X . . . X X X X . X \n",
      "X . X X X X . X X X . . X X X . . . . X \n",
      "X X X X X . . . . . . X X X . . X X X X \n",
      "X . . . . . X . X X X X . X X . . . X X \n",
      "X . X X X . X . . . . . . . . . X . . X \n",
      "X X X X . . X . X . X X X X X X X X . X \n",
      "X . . . . X X . X . . X X X . X . X X X \n",
      "X X . X . X . . X X . . . . . . . X X X \n",
      "X . . X . X . X X X X . X X . X X X . X \n",
      "X . X X X X . . X X . . . X X X X . . X \n",
      "X X X . X . . X X X . X . X X . X . X X \n",
      "X . . . . . X X . . . X . . . . . . . X \n",
      "X X . X X . X X . X . X . X . X . X X X \n",
      "X . . X X . X X . X . X . X X X . . . X \n",
      "X X . X X . X X . X . X . . . X X . X X \n",
      "X X . X X . X . . X X X . X X X . . . X \n",
      "X . . X X . X X . X X . . . X X . X . X \n",
      "X X X X X X X X X X X X X X X X X X O X \n",
      "\n"
     ]
    }
   ],
   "source": [
    "maze = QMaze(20)\n",
    "maze.reset()\n",
    "maze.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An epsilon-greedy policy\n",
    "We can combine our random policy and our greedy policy to make an improved policy that both explores its environment and exploits its current knowledge. An $\\epsilon$-greedy (epsilon-greedy) policy is one which exploits what it knows most of the time, but with probability $\\epsilon$ will instead select a random action to try.\n",
    "\n",
    "## Do we need to keep exploring once we are confident in the values of states?\n",
    "\n",
    "As our agent explores more, it becomes more confident in predicting how valuable any state is. Once it knows a lot, it should start to explore less and exploit what it knows more. That means that we should decrease epsilon over time.\n",
    "\n",
    "Let's implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epolicy = E_greedy_policy(1, 0.999)\n",
    "epolicy.reset()\n",
    "qlearning = Qlearning(epolicy,maze, 0.9, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent 1000 times\n",
    "turns = 1000\n",
    "for _ in range(turns):\n",
    "    s = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epolicy(s, qlearning.q_values)\n",
    "\n",
    "        # Get the next state and reward after performing the action\n",
    "        s_next, r, done = maze.step(action)\n",
    "\n",
    "        # Update the values of the q table\n",
    "        qlearning.update_values(s, s_next, action.value.index, r)\n",
    "\n",
    "        # Update the epsilon policy\n",
    "        epolicy.update_epsilon()\n",
    "\n",
    "        s = s_next\n",
    "\n",
    "# Get the best actions for each state\n",
    "best_actions = qlearning.new_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent by visualizing it\n",
    "state = maze.reset()\n",
    "timestep, rewards = 0, 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Get the best action for the current state\n",
    "    action_index = best_actions[state]\n",
    "    selected_action = None\n",
    "    for i in list(Action):\n",
    "        if i.value.index == action_index:\n",
    "            selected_action = i\n",
    "\n",
    "    # Perform the selected action\n",
    "    state, reward, done = maze.step(selected_action)\n",
    "    rewards += reward\n",
    "    \n",
    "    # Display the maze and action information\n",
    "    clear_output(wait=True)\n",
    "    maze.display()\n",
    "    print(f\"Time step: {timestep}\")\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"Action: {selected_action}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "\n",
    "    timestep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the agent after training\n",
    "\n",
    "total_timesteps, total_penalties = 0, 0\n",
    "total_rewards = 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = maze.reset()\n",
    "    timesteps, reward = 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Select the best action from the result of qlearning\n",
    "        action_index = best_actions[state]\n",
    "        selected_action = None\n",
    "        for i in list(Action):\n",
    "            if i.value.index == action_index:\n",
    "                selected_action = i\n",
    "                \n",
    "        # Perform selected action and get the next state and current reward\n",
    "        state, reward, done = maze.step(selected_action)\n",
    "        \n",
    "        # Keep track of the total reward\n",
    "        total_rewards += reward\n",
    "        \n",
    "        timesteps += 1\n",
    "\n",
    "    total_timesteps += timesteps\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_timesteps / episodes}\")\n",
    "print(f\"Average rewards per episode: {rewards / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
